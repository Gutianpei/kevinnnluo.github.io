# -*- coding: utf-8 -*-
"""320 Final Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WGzti7r3pxukca5KwWOwLnUDrb7Su4XX
"""

import pandas as pd
import numpy as np
import seaborn as sns
from bs4 import BeautifulSoup
import requests
import re
import matplotlib.pyplot as plt

# # !pip install fake_useragent 
# from fake_useragent import UserAgent
# ua = UserAgent()
# BASE_URL = 'https://www.cargurus.com/Cars/searchResults.action?zip=20742&inventorySearchWidgetType=AUTO&nonShippableBaseline=0&hideFreemiumBaseline=false&sourceContext=carGurusHomePageModel&distance=50&offset={}&maxResults=15&filtersModified=true'
# API_URL = 'https://www.cargurus.com/Cars/detailListingJson.action?inventoryListing={}'
# INDEX_URL = 'https://www.cargurus.com/'

# page = requests.get(INDEX_URL)
# soup = BeautifulSoup(page.content, 'html.parser')
# makes = [make.text for make in soup.find('optgroup', {'label':"All Makes"}).find_all('option', text=True)]
# links = []
# # print(ids)

# import random , os
# import multiprocessing as mp
# from queue import Empty
# import math
# import time
# print(mp.cpu_count())
# # session = requests.Session()
# def f(i) :
#     # for i in range(1,1001):
#     time.sleep(0.5)
#     headers = {'User-Agent': str(ua.random)}
#     page = requests.get(BASE_URL.format(15*i), headers=headers)
#     print(BASE_URL.format(15*i))
    
#     try:
#       return [a['id'] for a in page.json()]
#     except:
#       print(i)
#       return []
#     # print(1)
#     # soup = BeautifulSoup(page.content, 'html.parser')
#     # return [link['href'] for link in soup.findAll('a', {"class":"carBladeLink"})]

# y = range(1000)
# flatten = lambda l: [item for sublist in l for item in sublist]
# results = flatten(list(map(f,y)))
# print(results)
# print(len(results))
# print(len(set(results)))
# df = pd.DataFrame(columns=['id', 'price', 'mileage', 'hasVehicleHistoryReport', 'hasThirdPartyVehicleDamageData', 'accidentCount', 'ownerCount', 'hasVehicleHistory', 'make', 'model', 'year', 'trim', 'bodyStyle' ])
# df

# df.drop('Unnamed: 0', axis=1)
# from google.colab import files
# df.to_csv('filename.csv') 
# files.download('filename.csv')

# import json
# from pandas.io.json import json_normalize
# # for id in ids:
 
# def g(id) :
#   time.sleep(0.5)
#   try:
#     arr = []
#     data = json.loads(session.get(API_URL.format(id)).content)
#     listing = data['listing']
#     history = data['listing']['vehicleHistory']
#     # print(listing)
#     # print(history)
#     print(id)
#     keys = ['id', 'price', 'mileage']
#     arr += [listing.get(key) for key in keys]
#     keys = ['hasVehicleHistoryReport', 'hasThirdPartyVehicleDamageData', 'accidentCount', 'ownerCount', 'hasVehicleHistory']
#     arr += [history.get(key) for key in keys]
#     #print(arr)
#     info = data['autoEntityInfo']
#     keys = ['make', 'model', 'year', 'trim', 'bodyStyle']
#     arr += [info.get(key) for key in keys]
#     return arr
#   except:
#     print(id)
#     return None

# p = mp.Pool(8)

# res = p.map(g,results)

# for r in res:
#   df = df.append(pd.Series(r, index=df.columns),ignore_index=True)

# from google.colab import files
# df.to_csv('filename.csv') 
# files.download('filename.csv')

df = pd.read_csv("https://gist.githubusercontent.com/jzwang43/834fb0684b7f40256c7d7adbb444260a/raw/61b4efb307bb64d2385f1c763d20b80151e14a46/data.csv")
df = df[df['price'] < 100000]
# df = df.head(3000)

## Tidy up data
df_num = df.drop_duplicates()

## Transform every string into integer or float value (ie. true = 1)
df_num.dropna(inplace=True)
df_num=df_num.replace(to_replace= True,value = 1)
df_num=df_num.replace(to_replace= False,value = 0)

## Categorization
make = df.make.astype('category') 
df_num['make'] = make.cat.codes
model = df.model.astype('category') 
df_num['model'] = model.cat.codes
trim = df.trim.astype('category') 
df_num['trim'] = trim.cat.codes
bodyStyle = df.bodyStyle.astype('category') 
df_num['bodyStyle'] = bodyStyle.cat.codes
df_num

df[df['hasThirdPartyVehicleDamageData'] == False]

#  here we set the figure size to 15x8
plt.figure(figsize=(15, 8))
# plot two values price per year_model
plt.scatter(df.price, df.year)
plt.xlabel("price", fontsize=14)
plt.ylabel("year", fontsize=14)
plt.title("Scatter plot of price and year of model",fontsize=18)
plt.show()

f, ax = plt.subplots(figsize=(15, 12))
sns.stripplot(data = df, x='price', y='make', jitter=.1)
plt.show()

plt.figure(figsize=(17,8))
df.make.value_counts().nlargest(20).plot(kind='barh')
plt.xlabel('Make Frequency')
# plt.title("Frequency of TOP 20 Marks distribution",fontsize=18)
plt.show()

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
y = df_num['price'].values
X = df_num.drop('hasVehicleHistory',1)
X = X.drop('price',1)
X = X.drop('hasVehicleHistoryReport',1)
X = X.drop('hasThirdPartyVehicleDamageData',1)
X = X.drop(['id', 'trim'],1)
X = X.values
y =df_num['price']
X_train, X_test, y_train, y_test = train_test_split(df_num.drop('price', axis=1), y, test_size=0.3)
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

reg = LinearRegression().fit(X_train, y_train)

# predictions = reg.predict(X_test)
# from scipy import stats
# # predictions = predictions[(np.abs(stats.zscore(y_test)) < 3).all(axis=1)]
# plt.scatter(y_test, predictions)
# # plt.scatter(y_test, predictions)
# # print(y_test)
# plt.xlabel('True Values')
# plt.ylabel('Predictions')
# plt.xlim(0, 50000)
import pandas as pd
from sklearn import preprocessing

# x = df.values #returns a numpy array
min_max_scaler = preprocessing.MinMaxScaler()
X_scaled = min_max_scaler.fit_transform(X)
# df = pd.DataFrame(X_scaled)
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_validate, cross_val_predict, cross_val_score
scores = cross_val_score(reg, X_scaled, y, cv=5, scoring = 'r2')
scores

print(scores)

### Feature ranking
from sklearn.metrics import r2_score
!pip install eli5
import eli5
from eli5.sklearn import PermutationImportance

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators = 100,
                           n_jobs = -1,
                           oob_score = True,
                           bootstrap = True,
                           random_state = 42)
rf.fit(X_train, y_train)

perm = PermutationImportance(rf, cv = None, refit = False, n_iter = 50).fit(X_train, y_train)
perm.feature_importances_



"""hasVehicleHistory, hasVehicleHistoryReport and hasThirdPartyVehicleDamageData only have one unique value, we have to drop them to prevent a NaN loss score while training our model"""

## Neural Network
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

## Enable GPU
config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 4} ) 
sess = tf.Session(config=config) 
keras.backend.set_session(sess)

## Generate trainning data
y = df_num['price'].values
X = df_num.drop('hasVehicleHistory',1)
X = X.drop('price',1)
X = X.drop('hasVehicleHistoryReport',1)
X = X.drop('hasThirdPartyVehicleDamageData',1)
X = X.drop('id',1)
X = X.drop('accidentCount',1)
X = X.drop('ownerCount',1)
X = X.drop('model',1)
X = X.drop('trim',1)
X = X.values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

## Normalization
mean = X_train.mean(axis=0)
std = X_train.std(axis=0)
X_train -= mean
X_train /= std

X_test -= mean
X_test /= std

## Build neural network

def build_model():
	model = keras.Sequential()
	model.add(keras.layers.Dense(64, activation='relu', input_shape=(4,)))
	model.add(keras.layers.Dense(64, activation='relu'))
	model.add(keras.layers.Dense(1))
  ## Loss function and optimization
	model.compile(optimizer='adam', loss='mse', metrics=['mae'])
	return model



## Training the final model
# need a very very very very very large epochs, tranning may take more than 10 hrs
model = build_model()

# Set callback functions to early stop training and save the best model so far
callbacks = [EarlyStopping(monitor='loss', patience=200),
         ModelCheckpoint(filepath='best_model.h5', monitor='loss', save_best_only=True)]

model.fit(X_train, # Features
          y_train, # Target vector
                  epochs=30000, # Number of epochs
                  callbacks=callbacks, # Early stopping
                  verbose=1, # Print description after each epoch
                  batch_size=100) # Number of observations per batch
                  #validation_data=(test_features, test_target)) # Data for evaluation
# model.fit(X_train, y_train, epochs=10, batch_size=16, verbose=1)
test_mse_score, test_mae_score = model.evaluate(X_test, y_test)

## error score
print(test_mae_score)
print(test_mse_score)

# y_predict 
y_predict = model.predict(X_test)
y_predict

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_validate, cross_val_predict, cross_val_score

## SVM with GridSearch
## GridSearch will take forever I just give up
from sklearn.svm import SVR

# C = [0.001, 0.01, 0.1, 1, 10]
# gammas = [0.001, 0.01, 0.1, 1]
# kernels = ['rbf', 'sigmoid']
# param_grid = {'C': C, 'gamma' : gammas, 'kernel' : kernels}
# grid_search = GridSearchCV(SVC(), param_grid, cv=10)
# grid_search.fit(X, y)
# svm_hyper = grid_search.best_params_
svm = SVR(kernel='linear')
scores = cross_val_score(svm, X, y, cv=5, scoring = 'r2')
scores

# ## LinearDiscriminant
# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
# lda = LinearDiscriminantAnalysis()
# scores = cross_val_score(lda, X, y, cv=5,scoring = 'r2')
# scores

#KNN
from sklearn.neighbors import KNeighborsRegressor
grid_param = dict(n_neighbors=[i for i in range(1, 100)])
grid_search = GridSearchCV(KNeighborsRegressor(), grid_param, cv=10)
grid_search.fit(X, y)
print(grid_search.best_params_)
knn = KNeighborsRegressor(n_neighbors=grid_search.best_params_['n_neighbors'])
min_max_scaler = preprocessing.MinMaxScaler()
X_scaled = min_max_scaler.fit_transform(X)
#cross validation to get the percentage of correct prediction in each models
scores = cross_val_score(knn, X_scaled, y, cv=10,scoring = 'r2')
scores

# # df = pd.DataFrame(X_scaled)
# df_num.head(200)

from sklearn.ensemble import AdaBoostRegressor
regr = AdaBoostRegressor(random_state=0, n_estimators=20000)
scores = cross_val_score(regr, X_scaled, y, cv=10,scoring = 'r2')
scores

from sklearn import linear_model
clf = linear_model.Lasso(alpha=0.1)
scores = cross_val_score(clf, X, y, cv=10,scoring = 'r2')
scores

from sklearn.linear_model import Ridge
ridge = Ridge()
scores = cross_val_score(ridge, X, y, cv=10,scoring = 'r2')
scores

# from sklearn.datasets import load_boston
# from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor
# from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import VotingRegressor
# reg1 = GradientBoostingRegressor(random_state=1, n_estimators=10)
reg2 = RandomForestRegressor(random_state=1, n_estimators=10)
reg3 = LinearRegression()
reg4 = KNeighborsRegressor(n_neighbors=98)
ereg = VotingRegressor(estimators=[ ('rf', reg2), ('knn', reg4)])
ereg = ereg.fit(X, y)
scores = cross_val_score(ereg, X, y, cv=10,scoring = 'r2')
scores

y = df_num['price']
X = df_num.drop('hasVehicleHistory',1)
X = X.drop('price',1)
X = X.drop('hasVehicleHistoryReport',1)
X = X.drop('hasThirdPartyVehicleDamageData',1)
X = X.drop('id',1)
X

perm.feature_importances_

df

